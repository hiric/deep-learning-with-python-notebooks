# Ch02 神经网络的数学基础
（熟悉英文单词，方便阅读代码）
## 2.1 初识神经网络

在机器学习中
- 分类问题中的某个类叫做类（Class）
- 数据点（Data Point）叫做样本（Sample）
- 某个样本对应的类叫做标签（Label）
- 训练集（Training set）：模型使用训练集进行学习
- 测试集（Test set）：模型使用测试集进行测试

## 2.4 神经网络的“引擎”：基于梯度的优化

神经网络架构中层的权重，也叫做层的属性，或者叫做可训练参数。这些参数初始值可以使用随机初始化。
训练循环（training loop）：
1. 抽取训练样本x和对应目标y组成的数据批量；
2. 在x上运行网络[这一步叫做前向传播（forward pass）]，得到预测值y_pred；
3. 计算网络在这批数据上的损失，用于衡量y_pred和y之间的距离；
4. 更新网络的所有权重，使网络在这批数据上的损失略微下降

### 2.4.1 导数（Derivative）

导数：连续的光滑函数可以对某个点求导，导数就是该点的斜率。

### 2.4.2 梯度（Gradient）

梯度：张量运算的导数，是导数在多元函数上的推广，使用张量作为多元函数的输入，梯度就是该点的曲率（Curvature）

### 2.4.3 随机梯度下降

小批量随机梯度下降（Mini-batch Stochastic Gradient Descent，小批量SGD）：
1. 抽取训练样本和对应目标组成的数据批量
2. 在训练样本上训练网络，得到预测值
3. 计算网格在这批数据上的损失，用于衡量预测值和对应目标之间的距离
4. 计算损失相对于网络参数的梯度（一次反向传播）
5. 将参数沿着梯度的反方向移动一点，从而使这批数据上的损失减小一点

随机（stochastic）是指每批数据都是随机抽取的。

SGD的多种变体：带动量的SGD、Adagrad、RMSProp等等。这些变体通称为优化方法或者优化器（optimizer）。

带动量的随机梯度下降的实现：
```python
past_velocity = 0.  # 过去的速度
learning_rate = 0.01
momentum = 0.1  # 固定不变的动量因子
while loss > 0.01:
    w, loss, gradient = get_current_parameters()
    velocity = past_velocity * momentum - learning_rate * gradient
    w = w + momentum * velocity - learning_rate * gradient
    past_velocity = velocity
    update_parameter(w)
    pass
```
动量方法来源于物理学，实现过程不仅考虑当前的加速度（斜率值），还考虑当前的速度（来源于过去的速度和当前的加速度）
过完的速度值越大，对现在的速度影响越大

### 2.4.4 链式求导：反向传播算法
(ToSee：充分理解基于梯度的优化方法的工作原理）

TensorFlow 使用符号微分（symbolic differentiation）的现代框架来实现神经网络。
给定一个运算链，并且已知每个运算的导数，这些框架可以利用链式法则来计算这个运算链的梯度函数。

## 本章小结
- 学习是指找到一组模型参数，使得在给定的训练样本和对应目标上的损失函数最小化
- 学习的过程：随机选取包含训练样本和对应目标的数据批量，计算批量损失相对于网络参数的梯度，依据梯度的反方向移动网络参数（移动距离由学习率确定）
- 因为神经网络是一系列可以微分的张量计算，因此利用求导的链式法则得到梯度函数，这个函数将当前参数和当前数据批量映射为一个梯度值
- 损失和优化器
    - 损失是在训练过程中需要最小化的量，用于衡量当前任务是否成功解决
    - 优化器是使用损失梯度更新参数的具体形式
    