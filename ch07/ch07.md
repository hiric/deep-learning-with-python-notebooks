# ch07 高级的深度学习最佳实践

注：这章中多是Keras的高级使用技巧，没有利用实际的样本数据进行训练，所以也就没有太多执行代码的需要。
代码更多是使用上的介绍，没有太多实际的含义。后面可以考虑看看能不能找到更好的数据来实现。

## 7.1 使用 Keras 函数式 API 实现更加复杂的网络架构

- Sequential模型：只有一个输入和一个输出，网络是层的线性堆叠。（图7-1）
- 多模态输入（multimodal）：合并来自不同输入源的数据，并且使用不同类型的神经层处理不同类型的数据，
联合学习一个更加精确的数据模型。
- 预测输入数据的多个目标属性，即一个输入多个输出，不同输出之间具有相关性，一起学习可以得到更加丰富而且准确的表示。
- Inception系列网络：输入被多个并行的卷积分支所处理，然后将这些分支的输出合并为单个张量
- ResNet系列网络：向模型中添加残差连接（Residual Connection），即将前面的输出张量与后面的输出张量相加，
从而将前面的表示重新注入到下游的数据流中，防止信息处理过程中的信息损失。

### 7.1.4 层组成的有向无环图

1. Inception 模块：模块的堆叠，每个模块本身都是一个小型的独立网络，被分为多个并行分支。
    - 1x1卷积（逐点卷积）的作用：计算得到的特征能够将输入张量通道中的信息混合在一起，有助于区分开通道特征学习和空间特征学习。
        - 卷积能够在输入张量的每一个方块周围提取空间图块，并且对所有图块应用相同的变换。
2. 残差连接：向任何多于10层的模型中添加残差连接，都可能会有所帮助
    - 残差连接是让前面某层的输出作为后面某层的输入，从而在序列网络中有效地创造了一条捷径。
    - 前面层的输出与后面层的激活相加
        - 恒等残差连接：输出与激活是相同的形状，可以直接相加
        - 线性残差连接：输出与激活是不同的形状，可以利用1x1卷积将输出下采样为与激活具有相同的形状，再相加
        - 解决了深度学习中存在的问题
            - 表示瓶颈：在Sequential模型中，每个连续的表示层都构建于前一层之上，这意味着模型只能访问前一层激活中包含的信息。
            如果某一层太小（比如特征维度太低），那么模型将会受限于该层激活中可以塞入多少信息，即对后面的层来说造成了信息丢失。
            利用残差连接可以将较早的信息重新注入到下游数据中。
            - 梯度消失：反向传播是用于训练深度神经网络的主要算法，其工作原理是将来自输出损失的反馈信号向下传播到更底部的层。
            如果这个反馈信号的传播需要很多层，那么信号可能会变更非常微弱，甚至完全消失，导致网络无法训练。
                - LSTM 使用携带轨道解决这个问题。
                - 残差连接引入了一个纯纯属的信息拾轨道，与主要的层堆叠方向平等，从而有助于跨越任意深度的层来传播梯度。
        
   
### 7.1.5 共享层权重

应用模式：需要一个LSTM层处理两个输入，这个LSTM层的表示（即它的权重）是同时基于两个输入来学习的。
称作 连体 LSTM 或者 共享 LSTM 模型。

应用举例：需要评估两个句子之间的语义相似度的模型。输入是两个句子，输出是0~1之间的浮点数。
0表示两个句子完全不相关，1表示两个句子完全相同或者语义相同，那么就可以在对话系统中删除重复的自然语言查询。

### 7.1.6 将模型作为层

一个模型也可以看作一个更大的层。
例如：连体视觉模型（共享卷积基）

### 7.1.7 小结
- Sequential层只能够实现层的线性堆叠
- Keras Functional API 可以构建（多输入模型、多输出模型 和 具有复杂的内部网络拓扑结构的模型）
- 通过多次调用相同的层实例或者模型实例，可以在不同的处理分支之间重复使用层或者模型的权重

## 7.2 使用 Keras 回调函数 和 TensorBoard 来检查并且监控深度学习模型

### 7.2.1 训练过程中将回调函数作用于模型

回调函数（callback）：是在调用fit()函数时传入模型的一个对象（即实时特定方法的类实例），
回调函数在训练过程中的不同时间点都会被模型调用，可以访问关于模型状态与性能的所有可用数据，
还可以采取行动：中断训练、保存模型、加载一组不同的权重或者改变模型的状态。

回调函数的使用场景：
- 模型检查点（model checkpointing）：在训练过程中的不同时间点保存模型的当前权重
- 提前终止（early stopping）：如果验证损失不再改善，则中断训练
- 在训练过程中动态调节某些参数值：优化器的学习率
- 在训练过程中记录训练指标和验证指标，或者将模型学到的表示可视化（这些表示也在不断更新中）：
例如 Keras 的进度条就是一个回调函数

Keras的常用回调函数：
- keras.callbacks.ModelCheckpoint：设置模型的检查点
- keras.callbacks.EarlyStopping：确定模型中断训练，提前终止的条件
- keras.callbacks.LearningRateScheduler
- keras.callbacks.ReduceLROnPlateau
- keras.callbacks.CSVLogger

### 7.2.2 TensorBoard 简介：TensorFlow 的可视化框架

- 在训练过程中以可视化的方式监控指标
- 将模型架构可视化
- 将激活和梯度的直方图可视化
- 以三维的形式研究嵌入

安装好 tensorboard 工具包后，可执行文件会安装在 scripts 目录里面。
打开 Terminal 后如果无法执行 tensorboard 命令，就需要在 path 里面设置。
Windows 7 + PyCharm + AnaConda 环境下：
- 先用 AnaConda 安装好 TensorFlow，TensorBoard也会自动安装
- 目录（C:\ProgramData\Anaconda3\envs\“你的环境名称”\Scripts）内已经安装了tensorboard.exe
- 如果打开 Terminal 后如果无法执行 tensorboard 命令，就需要在 path 里面设置。
- 在 Terminal 内，进入 ch07 目录，执行 tensorboard --logdir=my_log_dir 就可以启动 tensorboard 服务器
- 打开系统给出的地址，例如：http://zYxTom:6006，就可以看到 TensorBoard 的可视化结果了
- ch0702_tensor_board.py 执行时会自动在 ch07 目录下生成 my_log_dir 目录，并且把日志文件记录在内

TensorBoard给出的信息：
- 模型的监控指标的原始曲线和平滑曲线
- 模型的计算图
- 模型参数的值分布情况
- 模型参数的值的直方图

注：如果对网络没有深入的理解，感觉图形给不了任何信息。 

### 7.2.3 小结

- Keras 回调函数是一种简单的控制模型的方法，可以在训练过程中监控模型的状态，并且根据模型状态自动采取行动
- TensorFlow 的 TensorBoard 是一种将模型活动可视化的好工具，可以通过 Keras 中的 TensorBoard 回调函数来使用

## 7.3 让模型性能发挥到极致

### 7.3.1 高级架构模式

1. 批标准化：在训练过程中均值和方差随时间发生变化，即自适应地将数据标准化。
    - 工作原理：训练过程中在内部保存已经读取的每批数据均值和方差的指数移动平均值
    - 主要效果：有助于梯度传播，允许更深的网络。
2. 深度可分离卷积（Depthwise Separable Convolution）：将空间特征学习和通道特征学习分开
    - 如果输入中的空间位置高度相关，而不同的通道之间相对独立，那么这个分离卷积会很有用
    - 需要的参数更少，计算量更小。

### 7.3.2 超参数优化

超参数（hyperparameter）：影响参数的参数。

超参数优化的过程：
1. 选择一组超参数（自动选择）
2. 构建相应的模型
3. 将模型在训练数据上拟合，并且衡量其在验证数据上的最终性能
4. 选择要尝试的下一组超参数（自动选择）
5. 重复上述过程
6. 基于不同组超参数模型的性能，选出最好的一组超参数
7. 衡量模型在最优组超参数上测试数据的性能

### 7.3.3 模型集成（Model Ensembling）

模型集成：将一系列不同模型的预测结果汇集到一起，从而得到更好的预测结果。
- 分类器集成：将一组分类器的预测结果汇集到一起。
    - 平均值预测
    - 加权平均预测
- 集成有效性的关键：分类器的多样性
    - 集成的每个模型要尽可能的好
    - 集成的每个模型相互之间要尽可能的不同
    
### 7.3.4 小结
- 构建高性能的深度卷积神经网络，需要使用残差连接、批标准化和深度可分离卷积。
    - 深度可分离卷积可能会取代普通卷积，因为它的表示效率更高
- 构建深度网络需要选择许多超参数和架构，这些选择共同决定了模型的性能。
    - Hyperopt 和 Hyperas 这两个库可能会对选择超参数有帮助
    - 进行超参数优化时，小心验证集过拟合
- 获得某项任务的最佳结果，只能通过多个模型的集成来实现
    - 利用加权平均进行集成通常能够取得足够好的效果
    - 最好的集成方法就是将尽可能不同的一组模型集成
        - 这组模型要有尽可能高的预测能力
        - 这组模型相互之间要有尽可能的不同（多样性）
        
## 本章总结
- 将模型构建为层组成的图
- 层的重复使用：层权重共享
- 模型的重复使用：模型模板
- 使用 Keras 回调函数在训练过程中监控模型，并且根据模型状态采取行动
- 使用 TensorBoard 将指标、激活直方图以及嵌入空间可视化
- 为了构建高性能的深度卷积神经网络，可以使用批标准化、深度可分离卷积和残差连接
- 为了获得更好的模型性能，可以使用超参数优化和模型集成